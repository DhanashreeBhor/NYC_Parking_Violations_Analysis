{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "217a7f9f-900f-4bb0-8292-a16ed68f2221",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./ConWidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b498bbb5-8c17-41bc-8802-80e3437cbf7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the numpy, pandas, datetime, matplotlib, & seaborn packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab7b01b7-1b4c-44e5-a033-cd4b149cb216",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adls_raw_path : abfs://bronze@azdac24we.dfs.core.windows.net/raw/\nAdls_silver_path : abfs://silver@azdac24we.dfs.core.windows.net/raw/\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:423)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1220)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:954)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:63)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:63)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:63)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:63)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:911)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:674)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:688)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:751)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:556)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:423)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1220)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:954)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:63)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:63)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:63)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:63)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:911)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:674)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:688)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:751)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:556)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Import the numpy, pandas, datetime, matplotlib, & seaborn packages\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import datetime\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0211ecbc-cf37-4b48-bb71-e78f96cecac1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(f\"{Adls_raw_path}/Parking_Violations_Issued_Fiscal_Year_2020.csv\", header = \"True\" )\n",
    "df1=df1.toDF(*(col.replace(' ','_') for col in df1.columns)).createOrReplaceTempView('df1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78eb54a2-309d-4bb6-955b-28be9db1d6e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select * from df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c8d872-2444-4ab0-bd63-b197e68aea8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"\"\"\n",
    "                  SELECT to_date(LEFT(REPLACE(trim(Issue_Date),'/','') , 8),'MMddyyyy') FROM df1\n",
    "                  \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b607e546-9f56-40c2-9575-c0925ef83d46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c89b207-cf46-40e2-b0dc-59ac7a5cfad9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# len(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ce7477-c023-47f1-9bdf-1e93d8f15f43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df11=df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3013425e-8e47-4210-96f4-7fba91a9937d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, to_date\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Date Transformation\").getOrCreate()\n",
    "\n",
    "# Step 1: Remove hidden spaces\n",
    "df11 = df11.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \" \", \"\"))\n",
    "\n",
    "# Step 2: Convert \"-\" to \"/\" and standardize date format to MM/dd/yyyy\n",
    "df11 = df11.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \"-\", \"/\"))\n",
    "\n",
    "# Since the datetime might be in different formats and lengths, first extract the relevant portion\n",
    "# Assuming the maximum length of the date string we're interested in is 10 characters (MM/dd/yyyy)\n",
    "df11 = df11.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \"(\\\\d{2}/\\\\d{2}/\\\\d{4}).*\", \"$1\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08f79496-4bb1-4869-8ccb-290954eb8431",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df11.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad1a9a74-b77b-49ff-9602-82ee98322493",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # get the record count\n",
    "# df11.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d99668-760a-4a5c-8bdd-c45cdc14ae13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, month\n",
    "\n",
    "# Assuming df51 is your DataFrame and has been prepared as per the previous transformations\n",
    "\n",
    "# Convert \"Issue Date\" column to date type (ensure this matches your column's name and format)\n",
    "df6 = df11.withColumn(\"Issue Date\", to_date(col(\"Issue Date\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "# Create or replace a temporary view to use SQL\n",
    "df6.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# SQL query to find the total number of tickets for each month and year\n",
    "month_year_wise_tickets = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    year(`Issue Date`) AS year, \n",
    "    month(`Issue Date`) AS month, \n",
    "    count(`Summons Number`) AS no_of_tickets \n",
    "FROM parking \n",
    "GROUP BY year, month \n",
    "ORDER BY year DESC, month DESC\n",
    "\"\"\")\n",
    "\n",
    "# # Show the results\n",
    "# month_year_wise_tickets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec8dfeab-a010-4801-a8b1-d04a5a615fa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df6.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f44bba9-00ab-4a89-99e6-4a3c7083c0af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.read.csv(f\"{Adls_raw_path}/Parking_Violations_Issued_Fiscal_Year_2021.csv\", header = \"True\")\n",
    "# display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f02b6b-1e25-4877-b3a3-b538f4020ecf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a141906-4bd3-4520-854e-144ad0f488ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# len(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9bb9ace-3f14-43b9-bde2-373b55af2dec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df21=df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d71b70-9932-443b-8b5c-ddf4bf202f6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, to_date\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Date Transformation\").getOrCreate()\n",
    "\n",
    "# Step 1: Remove hidden spaces\n",
    "df21 = df21.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \" \", \"\"))\n",
    "\n",
    "# Step 2: Convert \"-\" to \"/\" and standardize date format to MM/dd/yyyy\n",
    "df21 = df21.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \"-\", \"/\"))\n",
    "\n",
    "# Since the datetime might be in different formats and lengths, first extract the relevant portion\n",
    "# Assuming the maximum length of the date string we're interested in is 10 characters (MM/dd/yyyy)\n",
    "df21 = df21.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \"(\\\\d{2}/\\\\d{2}/\\\\d{4}).*\", \"$1\"))\n",
    "\n",
    "\n",
    "\n",
    "# # Show the transformed dates\n",
    "# df21.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8063181a-75fe-43cf-8f28-fa438682b09d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # get the record count\n",
    "# df21.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda7f6a3-08fd-4b9c-b1fd-4cdaf6bb5b17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, month\n",
    "\n",
    "# Assuming df51 is your DataFrame and has been prepared as per the previous transformations\n",
    "\n",
    "# Convert \"Issue Date\" column to date type (ensure this matches your column's name and format)\n",
    "df7 = df21.withColumn(\"Issue Date\", to_date(col(\"Issue Date\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "# Create or replace a temporary view to use SQL\n",
    "df6.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# SQL query to find the total number of tickets for each month and year\n",
    "month_year_wise_tickets = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    year(`Issue Date`) AS year, \n",
    "    month(`Issue Date`) AS month, \n",
    "    count(`Summons Number`) AS no_of_tickets \n",
    "FROM parking \n",
    "GROUP BY year, month \n",
    "ORDER BY year DESC, month DESC\n",
    "\"\"\")\n",
    "\n",
    "# # Show the results\n",
    "# month_year_wise_tickets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f85adf02-03d4-46fe-8af3-d9e669087ff9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3 = spark.read.csv(f\"{Adls_raw_path}/Parking_Violations_Issued_Fiscal_Year_2022.csv\", header = \"True\")\n",
    "# display(df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05950e20-281e-4091-8d59-49fe8a58a496",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# len(df3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d988b909-d8f0-4226-95ed-66e35551a313",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df31=df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817356ca-9511-49e5-af12-e7f492914e10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, to_date\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Date Transformation\").getOrCreate()\n",
    "\n",
    "\n",
    "# Step 1: Remove hidden spaces\n",
    "df31 = df31.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \" \", \"\"))\n",
    "\n",
    "# Step 2: Convert \"-\" to \"/\" and standardize date format to MM/dd/yyyy\n",
    "df31 = df31.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \"-\", \"/\"))\n",
    "\n",
    "# Since the datetime might be in different formats and lengths, first extract the relevant portion\n",
    "# Assuming the maximum length of the date string we're interested in is 10 characters (MM/dd/yyyy)\n",
    "df31 = df31.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \"(\\\\d{2}/\\\\d{2}/\\\\d{4}).*\", \"$1\"))\n",
    "\n",
    "\n",
    "\n",
    "# # Show the transformed dates\n",
    "# df31.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1684246-26d5-4b18-b362-fdffab066b68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, month\n",
    "\n",
    "# Assuming df51 is your DataFrame and has been prepared as per the previous transformations\n",
    "\n",
    "# Convert \"Issue Date\" column to date type (ensure this matches your column's name and format)\n",
    "df8 = df31.withColumn(\"Issue Date\", to_date(col(\"Issue Date\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "# Create or replace a temporary view to use SQL\n",
    "df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# SQL query to find the total number of tickets for each month and year\n",
    "month_year_wise_tickets = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    year(`Issue Date`) AS year, \n",
    "    month(`Issue Date`) AS month, \n",
    "    count(`Summons Number`) AS no_of_tickets \n",
    "FROM parking \n",
    "GROUP BY year, month \n",
    "ORDER BY year DESC, month DESC\n",
    "\"\"\")\n",
    "\n",
    "# # Show the results\n",
    "# month_year_wise_tickets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006707ab-fe73-4beb-912e-555851dde619",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4 = spark.read.csv(f\"{Adls_raw_path}/Parking_Violations_Issued_Fiscal_Year_2023.csv\", header = \"True\")\n",
    "# display(df4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a13a804-520c-431e-b689-aa512797b92a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# len(df4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7e46e26-4930-49e7-81f5-ebd88a8b438b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df41=df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f5f4cc-6395-440d-8dab-2c7b81a28858",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, to_date\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Date Transformation\").getOrCreate()\n",
    "\n",
    "# Step 1: Remove hidden spaces\n",
    "df41 = df41.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \" \", \"\"))\n",
    "\n",
    "# Step 2: Convert \"-\" to \"/\" and standardize date format to MM/dd/yyyy\n",
    "df41 = df41.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \"-\", \"/\"))\n",
    "\n",
    "# Since the datetime might be in different formats and lengths, first extract the relevant portion\n",
    "# Assuming the maximum length of the date string we're interested in is 10 characters (MM/dd/yyyy)\n",
    "df41 = df41.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \"(\\\\d{2}/\\\\d{2}/\\\\d{4}).*\", \"$1\"))\n",
    "\n",
    "\n",
    "# # Show the transformed dates\n",
    "# df41.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfaaa67f-5ad7-4d5f-b885-0ab873592d92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, month\n",
    "\n",
    "# Assuming df51 is your DataFrame and has been prepared as per the previous transformations\n",
    "\n",
    "# Convert \"Issue Date\" column to date type (ensure this matches your column's name and format)\n",
    "df9 = df41.withColumn(\"Issue Date\", to_date(col(\"Issue Date\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "# Create or replace a temporary view to use SQL\n",
    "df9.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# SQL query to find the total number of tickets for each month and year\n",
    "month_year_wise_tickets = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    year(`Issue Date`) AS year, \n",
    "    month(`Issue Date`) AS month, \n",
    "    count(`Summons Number`) AS no_of_tickets \n",
    "FROM parking \n",
    "GROUP BY year, month \n",
    "ORDER BY year DESC, month DESC\n",
    "\"\"\")\n",
    "\n",
    "# # Show the results\n",
    "# month_year_wise_tickets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc0b563-abdd-4f36-88e1-fd4be62a95b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df5 = spark.read.csv(f\"{Adls_raw_path}/Parking_Violations_Issued_Fiscal_Year_2024.csv\", header = \"True\")\n",
    "# display(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6a65844-7640-4962-a802-16a965ac8958",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# len(df5.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5736507e-b838-4673-965e-8a965d5c8407",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df51=df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7bd715a-8c15-4393-bf7c-5c9b5e121d24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, to_date\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Date Transformation\").getOrCreate()\n",
    "\n",
    "# # Sample DataFrame creation (assuming df is your DataFrame with 'Issue Date')\n",
    "# data = [\n",
    "#     (\"05-08-1972 00:00\",),\n",
    "#     (\"08/29/1977 12:00:00 AM\",),\n",
    "#     # Add the rest of your dates here...\n",
    "#     (\"01/24/2000 12:00:00 AM\",)\n",
    "# ]\n",
    "# df = spark.createDataFrame(data, [\"Issue Date\"])\n",
    "\n",
    "# Step 1: Remove hidden spaces\n",
    "df51 = df51.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \" \", \"\"))\n",
    "\n",
    "# Step 2: Convert \"-\" to \"/\" and standardize date format to MM/dd/yyyy\n",
    "df51 = df51.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \"-\", \"/\"))\n",
    "\n",
    "# Since the datetime might be in different formats and lengths, first extract the relevant portion\n",
    "# Assuming the maximum length of the date string we're interested in is 10 characters (MM/dd/yyyy)\n",
    "df51 = df51.withColumn(\"Issue Date\", regexp_replace(col(\"Issue Date\"), \"(\\\\d{2}/\\\\d{2}/\\\\d{4}).*\", \"$1\"))\n",
    "\n",
    "\n",
    "# # Show the transformed dates\n",
    "# df51.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a61b732-2151-4db7-b5d4-8df9e6a8d28a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, month\n",
    "\n",
    "# Assuming df51 is your DataFrame and has been prepared as per the previous transformations\n",
    "\n",
    "# Convert \"Issue Date\" column to date type (ensure this matches your column's name and format)\n",
    "df10 = df51.withColumn(\"Issue Date\", to_date(col(\"Issue Date\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "# Create or replace a temporary view to use SQL\n",
    "df10.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# SQL query to find the total number of tickets for each month and year\n",
    "month_year_wise_tickets = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    year(`Issue Date`) AS year, \n",
    "    month(`Issue Date`) AS month, \n",
    "    count(`Summons Number`) AS no_of_tickets \n",
    "FROM parking \n",
    "GROUP BY year, month \n",
    "ORDER BY year DESC, month DESC\n",
    "\"\"\")\n",
    "\n",
    "# # Show the results\n",
    "# month_year_wise_tickets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c8d404e-69a5-44ef-967d-8ecd58601941",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c9b6cc1-cf28-4449-88a1-28324bb4be5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Union all the DataFrames together to get the final df\n",
    "\n",
    "final_df = df6.unionAll(df7).unionAll(df8).unionAll(df9).unionAll(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94d5d27-8530-4493-a119-2020bb0d9e04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total count of columns\n",
    "len(final_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7af440d1-b4a4-47b1-84db-d8d443a29361",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total count of records\n",
    "final_df.counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aa4c466-95e3-45f5-af3a-60ee5ae2bc4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Summons Number: string (nullable = true)\n |-- Plate ID: string (nullable = true)\n |-- Registration State: string (nullable = true)\n |-- Plate Type: string (nullable = true)\n |-- Issue Date: date (nullable = true)\n |-- Violation Code: string (nullable = true)\n |-- Vehicle Body Type: string (nullable = true)\n |-- Vehicle Make: string (nullable = true)\n |-- Issuing Agency: string (nullable = true)\n |-- Street Code1: string (nullable = true)\n |-- Street Code2: string (nullable = true)\n |-- Street Code3: string (nullable = true)\n |-- Vehicle Expiration Date: string (nullable = true)\n |-- Violation Location: string (nullable = true)\n |-- Violation Precinct: string (nullable = true)\n |-- Issuer Precinct: string (nullable = true)\n |-- Issuer Code: string (nullable = true)\n |-- Issuer Command: string (nullable = true)\n |-- Issuer Squad: string (nullable = true)\n |-- Violation Time: string (nullable = true)\n |-- Time First Observed: string (nullable = true)\n |-- Violation County: string (nullable = true)\n |-- Violation In Front Of Or Opposite: string (nullable = true)\n |-- House Number: string (nullable = true)\n |-- Street Name: string (nullable = true)\n |-- Intersecting Street: string (nullable = true)\n |-- Date First Observed: string (nullable = true)\n |-- Law Section: string (nullable = true)\n |-- Sub Division: string (nullable = true)\n |-- Violation Legal Code: string (nullable = true)\n |-- Days Parking In Effect: string (nullable = true)\n |-- From Hours In Effect: string (nullable = true)\n |-- To Hours In Effect: string (nullable = true)\n |-- Vehicle Color: string (nullable = true)\n |-- Unregistered Vehicle?: string (nullable = true)\n |-- Vehicle Year: string (nullable = true)\n |-- Meter Number: string (nullable = true)\n |-- Feet From Curb: string (nullable = true)\n |-- Violation Post Code: string (nullable = true)\n |-- Violation Description: string (nullable = true)\n |-- No Standing or Stopping Violation: string (nullable = true)\n |-- Hydrant Violation: string (nullable = true)\n |-- Double Parking Violation: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# checking the schema of the table\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938fdf58-75ba-4b71-b015-a816a8b6bd9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cb70ba3-49a9-44c2-b187-fa4ef6d243b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import col, to_date, year, month\n",
    "\n",
    "# # Assuming df51 is your DataFrame and has been prepared as per the previous transformations\n",
    "\n",
    "# # Convert \"Issue Date\" column to date type (ensure this matches your column's name and format)\n",
    "# df = df.withColumn(\"Issue Date\", to_date(col(\"Issue Date\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "# # Create or replace a temporary view to use SQL\n",
    "# df.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# # SQL query to find the total number of tickets for each month and year\n",
    "# month_year_wise_tickets = spark.sql(\"\"\"\n",
    "# SELECT \n",
    "#     year(`Issue Date`) AS year, \n",
    "#     month(`Issue Date`) AS month, \n",
    "#     count(`Summons Number`) AS no_of_tickets \n",
    "# FROM parking \n",
    "# GROUP BY year, month \n",
    "# ORDER BY year DESC, month DESC\n",
    "# \"\"\")\n",
    "\n",
    "# # Show the results\n",
    "# month_year_wise_tickets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eaabaf2-b826-4964-b475-0c695dec1363",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+--------------+------------+------------+------------+-----------------------+------------------+------------------+---------------+-----------+--------------+------------+--------------+-------------------+----------------+---------------------------------+------------+-----------+-------------------+-------------------+-----------+------------+--------------------+----------------------+--------------------+------------------+-------------+---------------------+------------+------------+--------------+-------------------+---------------------+---------------------------------+-----------------+------------------------+\n|Summons Number|Plate ID|Registration State|Plate Type|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Issuing Agency|Street Code1|Street Code2|Street Code3|Vehicle Expiration Date|Violation Location|Violation Precinct|Issuer Precinct|Issuer Code|Issuer Command|Issuer Squad|Violation Time|Time First Observed|Violation County|Violation In Front Of Or Opposite|House Number|Street Name|Intersecting Street|Date First Observed|Law Section|Sub Division|Violation Legal Code|Days Parking In Effect|From Hours In Effect|To Hours In Effect|Vehicle Color|Unregistered Vehicle?|Vehicle Year|Meter Number|Feet From Curb|Violation Post Code|Violation Description|No Standing or Stopping Violation|Hydrant Violation|Double Parking Violation|\n+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+--------------+------------+------------+------------+-----------------------+------------------+------------------+---------------+-----------+--------------+------------+--------------+-------------------+----------------+---------------------------------+------------+-----------+-------------------+-------------------+-----------+------------+--------------------+----------------------+--------------------+------------------+-------------+---------------------+------------+------------+--------------+-------------------+---------------------+---------------------------------+-----------------+------------------------+\n|             0|      17|                16|        16|        16|            17|            75537|       49492|            17|          17|          17|          17|                     17|          11141461|                17|             17|         17|      11108142|    11610988|           635|           26204435|          115543|                         11245026|    11362771|       3017|           13992475|                 19|         19|        4230|            16390805|              11242681|            18162819|          18162834|      1839728|             26592381|          20|    24869284|            20|           12516883|              5877039|                         27498810|         27498810|                27498810|\n+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+--------------+------------+------------+------------+-----------------------+------------------+------------------+---------------+-----------+--------------+------------+--------------+-------------------+----------------+---------------------------------+------------+-----------+-------------------+-------------------+-----------+------------+--------------------+----------------------+--------------------+------------------+-------------+---------------------+------------+------------+--------------+-------------------+---------------------+---------------------------------+-----------------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# printing null values in the table \n",
    "from pyspark.sql.functions import col, sum as pyspark_sum\n",
    "\n",
    "# Replace 'df' with your DataFrame\n",
    "null_counts = df.select([pyspark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "\n",
    "# Display the null counts\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d892ac6-a460-4804-9545-cb0ac945a19c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+--------------+------------+------------+------------+-----------------------+------------------+---------------+-----------+--------------+----------------+-----------+-------------------+-----------+------------+-------------+------------+--------------+\n|Summons Number|Plate ID|Registration State|Plate Type|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Issuing Agency|Street Code1|Street Code2|Street Code3|Vehicle Expiration Date|Violation Precinct|Issuer Precinct|Issuer Code|Violation Time|Violation County|Street Name|Date First Observed|Law Section|Sub Division|Vehicle Color|Vehicle Year|Feet From Curb|\n+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+--------------+------------+------------+------------+-----------------------+------------------+---------------+-----------+--------------+----------------+-----------+-------------------+-----------+------------+-------------+------------+--------------+\n|    1477633194|  J58JKX|                NJ|       PAS|1972-05-08|            16|              SDN|       HONDA|             P|        8730|        5130|        5280|                      0|                72|            504|     342924|         0523P|               K|      43 ST|                  0|        408|          E2|           BK|           0|             0|\n+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+--------------+------------+------------+------------+-----------------------+------------------+---------------+-----------+--------------+----------------+-----------+-------------------+-----------+------------+-------------+------------+--------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "# Calculate the threshold for 80% non-null values\n",
    "threshold = int(df.count() * 0.8)\n",
    "\n",
    "# List of columns that meet the condition\n",
    "good_columns = [col for col in df.columns if df.filter(df[col].isNotNull()).count() >= threshold]\n",
    "\n",
    "# Select these columns\n",
    "df1 = df.select(*good_columns)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caa1ee05-92e1-420e-8e8b-8d8e84026748",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Summons Number: string (nullable = true)\n |-- Plate ID: string (nullable = true)\n |-- Registration State: string (nullable = true)\n |-- Plate Type: string (nullable = true)\n |-- Issue Date: date (nullable = true)\n |-- Violation Code: string (nullable = true)\n |-- Vehicle Body Type: string (nullable = true)\n |-- Vehicle Make: string (nullable = true)\n |-- Issuing Agency: string (nullable = true)\n |-- Street Code1: string (nullable = true)\n |-- Street Code2: string (nullable = true)\n |-- Street Code3: string (nullable = true)\n |-- Vehicle Expiration Date: string (nullable = true)\n |-- Violation Precinct: string (nullable = true)\n |-- Issuer Precinct: string (nullable = true)\n |-- Issuer Code: string (nullable = true)\n |-- Violation Time: string (nullable = true)\n |-- Violation County: string (nullable = true)\n |-- Street Name: string (nullable = true)\n |-- Date First Observed: string (nullable = true)\n |-- Law Section: string (nullable = true)\n |-- Sub Division: string (nullable = true)\n |-- Vehicle Color: string (nullable = true)\n |-- Vehicle Year: string (nullable = true)\n |-- Feet From Curb: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Schema of the table\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa5f18f7-bb4e-49bb-9de6-5b3fa8566b46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selecting necessary column\n",
    "columns_to_select = ['Summons Number','Plate ID','Registration State','Plate Type','Issue Date','Violation Code','Vehicle Body Type','Vehicle Make','Issuer Precinct','Issuer Code','Violation Time','Violation County']\n",
    "\n",
    "# Select the specified columns\n",
    "df2 = df1.select(*columns_to_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5616babd-a8cd-43ef-81f9-c7baaca6bd58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define a function to apply the renaming logic\n",
    "def rename_column(name):\n",
    "    return name.lower().strip().replace(' ', '_')\n",
    "\n",
    "# Apply the renaming logic to each column\n",
    "for column in df.columns:\n",
    "    df2 = df2.withColumnRenamed(column, rename_column(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21a5f93e-d99b-4ccd-a144-ddaede17b697",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+---------------+-----------+--------------+----------------+\n|summons_number|plate_id|registration_state|plate_type|issue_date|violation_code|vehicle_body_type|vehicle_make|issuer_precinct|issuer_code|violation_time|violation_county|\n+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+---------------+-----------+--------------+----------------+\n|    1477633194|  J58JKX|                NJ|       PAS|1972-05-08|            16|              SDN|       HONDA|            504|     342924|         0523P|               K|\n+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+---------------+-----------+--------------+----------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "df2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "799b57d5-33f2-4dbf-81c2-4d3a14dca1c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicate rows, keeping only the first occurrence of each set of duplicated rows\n",
    "df3 = df2.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01fb0552-4c68-4786-9f25-65af82652a6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Count the number of duplicate rows by grouping and counting\n",
    "duplicate_count = df3.groupBy(df3.columns).count().where(F.col('count') > 1).count()\n",
    "\n",
    "# Display the total number of duplicate rows\n",
    "print(\"Total number of duplicate rows:\", duplicate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "470cf5d5-9281-4ac5-91eb-ab47f1424426",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of records including null values:  27498798\nCount of records excluding null values:  27278034\n"
     ]
    }
   ],
   "source": [
    "# Check count before dorping null values\n",
    "print('Count of records including null values: ',df3.count())\n",
    "\n",
    "# Drop rows with missing values in the columns\n",
    "df3 = df3.na.drop()\n",
    "\n",
    "# Check count after dorping null values\n",
    "print('Count of records excluding null values: ',df3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e248b361-0cb4-416e-bfd1-61f8cb4abf35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+---------------+-----------+--------------+----------------+\n|summons_number|plate_id|registration_state|plate_type|issue_date|violation_code|vehicle_body_type|vehicle_make|issuer_precinct|issuer_code|violation_time|violation_county|\n+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+---------------+-----------+--------------+----------------+\n|             0|       0|                 0|         0|         0|             0|                0|           0|              0|          0|             0|               0|\n+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+---------------+-----------+--------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# printing null values in the table \n",
    "from pyspark.sql.functions import col, sum as pyspark_sum\n",
    "\n",
    "# Replace 'df3' with your DataFrame\n",
    "null_counts = df3.select([pyspark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df3.columns])\n",
    "\n",
    "# Display the null counts\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df4d30eb-2c50-4864-bad1-cfe813bfeb91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3.write.format(\"delta\").mode('overWrite').option('overwriteSchema', 'true').save('abfs://silver@azdac24we.dfs.core.windows.net/raw/track1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d10c9db2-759f-4645-bd35-90d13976d9ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df3.write.format(\"delta\").mode('overWrite').option('overwriteSchema', 'true').save('abfs://silver@azdac24we.dfs.core.windows.net/raw/track2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82617ea8-52e0-457b-923a-7407974ece86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- summons_number: string (nullable = true)\n |-- plate_id: string (nullable = true)\n |-- registration_state: string (nullable = true)\n |-- plate_type: string (nullable = true)\n |-- issue_date: date (nullable = true)\n |-- violation_code: string (nullable = true)\n |-- vehicle_body_type: string (nullable = true)\n |-- vehicle_make: string (nullable = true)\n |-- issuer_precinct: string (nullable = true)\n |-- issuer_code: string (nullable = true)\n |-- violation_time: string (nullable = true)\n |-- violation_county: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# # Import SparkSession\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Create a SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"CSV Reader\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# # Read CSV file into DataFrame\n",
    "# df4 = spark.read.format(\"delta\") \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .load(\"abfs://silver@azdac24we.dfs.core.windows.net/raw/track1/\")\n",
    "\n",
    "# # Show the DataFrame schema\n",
    "# df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7767e539-41df-4f59-9579-af522c8807ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4=df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2231023-5a9f-42d1-84df-fb3a5abf9338",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count before dropping duplicate summon numbers :  27278034\nCount before dropping duplicate summon numbers :  27278034\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate values based on the summons number column\n",
    "\n",
    "# Check the count before dropping duplicate summon numbers\n",
    "print('Count before dropping duplicate summon numbers : ', df4.count())\n",
    "\n",
    "# drop duplicate summon numbers\n",
    "df4.select('summons_number').dropDuplicates()\n",
    "\n",
    "# Check the count after dropping duplicate summon numbers\n",
    "print('Count before dropping duplicate summon numbers : ', df4.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "118f35d6-dfa1-4dc8-b913-3cc03a579c6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df5=df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc2c1f75-eefd-4979-9385-793842cfdafa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n|year|no_of_tickets|\n+----+-------------+\n|2067|            1|\n|2066|            1|\n|2061|            2|\n|2050|            2|\n|2049|            1|\n|2046|            1|\n|2040|            1|\n|2035|            1|\n|2034|            1|\n|2033|            7|\n|2032|            1|\n|2031|           20|\n|2030|           34|\n|2029|           12|\n|2028|           25|\n|2027|           29|\n|2026|           52|\n|2025|          121|\n|2024|          288|\n|2023|      6924642|\n|2022|      2165917|\n|2021|      8022169|\n|2020|      7397006|\n|2019|      2764508|\n|2018|          312|\n|2017|         2301|\n|2016|           48|\n|2015|           16|\n|2014|           35|\n|2013|           28|\n|2012|           49|\n|2011|           25|\n|2010|           70|\n|2009|            7|\n|2008|            2|\n|2007|            3|\n|2006|            5|\n|2005|            4|\n|2004|            6|\n|2003|            2|\n|2002|           10|\n|2001|            7|\n|2000|          246|\n|1995|            1|\n|1991|            1|\n|1990|            5|\n|1989|            1|\n|1988|            1|\n|1981|            1|\n|1977|            1|\n+----+-------------+\nonly showing top 50 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Assuming df51 is your DataFrame and has been prepared as per the previous transformations\n",
    "\n",
    "# Convert \"Issue Date\" column to date type (ensure this matches your column's name and format)\n",
    "df5 = df5.withColumn(\"issue_date\", to_date(col(\"issue_date\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "# Create or replace a temporary view to use SQL\n",
    "df5.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# # SQL query to find the total number of tickets for each month and year\n",
    "# month_year_wise_tickets = spark.sql(\"\"\"\n",
    "# SELECT \n",
    "#     year(`issue_date`) AS year, \n",
    "#     month(`issue_date`) AS month, \n",
    "#     count(`summons_number`) AS no_of_tickets \n",
    "# FROM parking \n",
    "# GROUP BY year, month \n",
    "# ORDER BY year DESC, month DESC\n",
    "# \"\"\")\n",
    "\n",
    "# # Show the results\n",
    "# month_year_wise_tickets.show(150)\n",
    "\n",
    "# SQL query to find the total number of tickets for each year\n",
    "year_wise_tickets = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    year(`issue_date`) AS year, \n",
    "    count(`summons_number`) AS no_of_tickets \n",
    "FROM parking \n",
    "GROUP BY year \n",
    "ORDER BY year DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "year_wise_tickets.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10484370-0a66-46b9-ac11-3d384b6c5879",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# month_year_wise_tickets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f232e31e-a3bd-48bc-a31b-24cb90020d03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n|year|no_of_tickets|\n+----+-------------+\n|2067|            1|\n|2066|            1|\n|2061|            2|\n|2050|            2|\n|2049|            1|\n|2046|            1|\n|2040|            1|\n|2035|            1|\n|2034|            1|\n|2033|            7|\n|2032|            1|\n|2031|           20|\n|2030|           34|\n|2029|           12|\n|2028|           25|\n|2027|           29|\n|2026|           52|\n|2025|          121|\n|2024|          288|\n|2023|      6924642|\n|2022|      2165917|\n|2021|      8022169|\n|2020|      7397006|\n|2019|      2764508|\n|2018|          312|\n|2017|         2301|\n|2016|           48|\n|2015|           16|\n|2014|           35|\n|2013|           28|\n|2012|           49|\n|2011|           25|\n|2010|           70|\n|2009|            7|\n|2008|            2|\n|2007|            3|\n|2006|            5|\n|2005|            4|\n|2004|            6|\n|2003|            2|\n|2002|           10|\n|2001|            7|\n|2000|          246|\n|1995|            1|\n|1991|            1|\n|1990|            5|\n|1989|            1|\n|1988|            1|\n|1981|            1|\n|1977|            1|\n+----+-------------+\nonly showing top 50 rows\n\n"
     ]
    }
   ],
   "source": [
    "# # SQL query to find the total number of tickets for each year\n",
    "# year_wise_tickets = spark.sql(\"\"\"\n",
    "# SELECT \n",
    "#     year(`issue_date`) AS year, \n",
    "#     count(`summons_number`) AS no_of_tickets \n",
    "# FROM parking \n",
    "# GROUP BY year \n",
    "# ORDER BY year DESC\n",
    "# \"\"\")\n",
    "\n",
    "# # Show the results\n",
    "# year_wise_tickets.show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "252537e0-61dd-4380-ac61-909dd3125359",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_wise_tickets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbb266ba-84b8-426d-acd7-e1229e99ec3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Following can be inferred from the above results :\n",
    "\n",
    "- The data contains information about the parking tickets for 53 years between 1977 to 2067\n",
    "\n",
    "- 2021 has maximum number of parking tickets followed by 2020\n",
    "\n",
    "- Since we would only be analysing 2020 to 2024 related data, let us filter out only 2010 to 2024 related data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9691b710-eaf0-44b3-8b00-77ce759e6156",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e6092ab-1da1-405f-9a0b-7876d9d8d2b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "002bed04-b109-4c09-a49f-b5583390539c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27265563\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# # Ensure Issue_Date is in the correct date format, if it's not already\n",
    "# # This step is necessary only if Issue_Date is still a string\n",
    "# df = df.withColumn(\"Issue_Date\", to_date(\"Issue_Date\", \"MM/dd/yyyy\"))\n",
    "\n",
    "# Now, filter using Spark SQL or DataFrame API\n",
    "# Spark SQL:\n",
    "# parking_filtered = spark.sql(\"\"\"\n",
    "# SELECT * \n",
    "# FROM parking\n",
    "# WHERE Issue_Date BETWEEN to_date('2019-06-01', 'yyyy-MM-dd') AND to_date('2024-01-31', 'yyyy-MM-dd')\n",
    "# \"\"\")\n",
    "\n",
    "# Or using DataFrame API:\n",
    "parking_filtered = df5.filter((col(\"Issue_Date\") >= lit(\"2019-06-01\")) & (col(\"Issue_Date\") <= lit(\"2024-01-31\")))\n",
    "\n",
    "# Count how many rows meet this criteria\n",
    "print(parking_filtered.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13bfc461-dd55-42ad-bbfb-3edde11c7406",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+---------------+-----------+--------------+----------------+\n|summons_number|plate_id|registration_state|plate_type|issue_date|violation_code|vehicle_body_type|vehicle_make|issuer_precinct|issuer_code|violation_time|violation_county|\n+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+---------------+-----------+--------------+----------------+\n|    1447619985|  6JX427|                NY|       PAS|2019-06-01|            24|              SDN|        HYUN|            836|        310|         1105A|               Q|\n|    1447627453| KWG0481|                PA|       PAS|2019-06-01|            83|             SUBN|        FORD|            835|        107|         1218A|               Q|\n|    1447627430| FFW3298|                NY|       PAS|2019-06-01|            20|              SDN|       TOYOT|            835|        107|         1203A|               Q|\n|    1447636521| CBZJ932|                TX|       PAS|2019-06-01|            24|              SDN|       ACURA|            835|        670|         0500P|               Q|\n|    1447627441| KWG0481|                PA|       PAS|2019-06-01|            20|              SDN|        FORD|            835|        107|         1209A|               Q|\n+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+---------------+-----------+--------------+----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "parking_filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b3488e9-7dae-4d6e-8caf-feaf23410be7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df7=parking_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a26c37f-89fc-4c03-b2e3-2c12c8978c11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df7.write.format(\"delta\").mode('overWrite').option('overwriteSchema', 'true').save('abfs://silver@azdac24we.dfs.core.windows.net/raw/track2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c2eaffc-9b18-49c9-a42b-7e4666f097db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8=df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62098b8c-23bf-4062-8a1b-d4674a2b940a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n|year|no_of_tickets|\n+----+-------------+\n|2023|      6924642|\n|2022|      2165917|\n|2021|      8022169|\n|2020|      7397006|\n|2019|      2755829|\n+----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Assuming df51 is your DataFrame and has been prepared as per the previous transformations\n",
    "\n",
    "# Convert \"Issue Date\" column to date type (ensure this matches your column's name and format)\n",
    "# df8 = df8.withColumn(\"Issue Date\", to_date(col(\"Issue Date\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "# Create or replace a temporary view to use SQL\n",
    "df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# # SQL query to find the total number of tickets for each month and year\n",
    "# month_year_wise_tickets = spark.sql(\"\"\"\n",
    "# SELECT \n",
    "#     year(`issue_date`) AS year, \n",
    "#     month(`issue_date`) AS month, \n",
    "#     count(`summons_number`) AS no_of_tickets \n",
    "# FROM parking \n",
    "# GROUP BY year, month \n",
    "# ORDER BY year DESC, month DESC\n",
    "# \"\"\")\n",
    "\n",
    "# # Show the results\n",
    "# month_year_wise_tickets.count()\n",
    "\n",
    "# SQL query to find the total number of tickets for each year\n",
    "year_wise_tickets = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    year(`issue_date`) AS year, \n",
    "    count(`summons_number`) AS no_of_tickets \n",
    "FROM parking \n",
    "GROUP BY year \n",
    "ORDER BY year DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "year_wise_tickets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62632cb9-094a-4e12-a038-081f9067671b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------+\n|year|month|no_of_tickets|\n+----+-----+-------------+\n|2023|   12|           24|\n|2023|   11|      1085537|\n|2023|   10|      1394351|\n|2023|    9|      1252657|\n|2023|    8|      1468790|\n|2023|    7|      1492489|\n|2023|    6|       229227|\n|2023|    5|         1367|\n|2023|    4|          100|\n|2023|    3|           27|\n|2023|    2|           16|\n|2023|    1|           57|\n|2022|   12|           60|\n|2022|   11|           72|\n|2022|   10|           78|\n|2022|    9|           73|\n|2022|    8|          184|\n|2022|    7|       986177|\n|2022|    6|       298578|\n|2022|    5|          687|\n|2022|    4|          170|\n|2022|    3|           69|\n|2022|    2|           21|\n|2022|    1|       879748|\n|2021|   12|      1155348|\n|2021|   11|      1278512|\n|2021|   10|      1340639|\n|2021|    9|      1185095|\n|2021|    8|      1414256|\n|2021|    7|      1398465|\n|2021|    6|       249191|\n|2021|    5|          207|\n|2021|    4|           50|\n|2021|    3|           33|\n|2021|    2|            8|\n|2021|    1|          365|\n|2020|   12|       641819|\n|2020|   11|      1212914|\n|2020|   10|      1411506|\n|2020|    9|      1445270|\n|2020|    8|      1393314|\n|2020|    7|      1042353|\n|2020|    6|       237543|\n|2020|    5|        11854|\n|2020|    4|          104|\n|2020|    3|          232|\n|2020|    2|           52|\n|2020|    1|           45|\n|2019|   12|           16|\n|2019|   11|           11|\n|2019|   10|           22|\n|2019|    9|        66483|\n|2019|    8|      1318797|\n|2019|    7|      1150925|\n|2019|    6|       219575|\n+----+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# month_year_wise_tickets.show(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e07106-87a3-4e76-9b0e-d6d542902e9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # SQL query to find the total number of tickets for each year\n",
    "# year_wise_tickets = spark.sql(\"\"\"\n",
    "# SELECT \n",
    "#     year(`issue_date`) AS year, \n",
    "#     count(`summons_number`) AS no_of_tickets \n",
    "# FROM parking \n",
    "# GROUP BY year \n",
    "# ORDER BY year DESC\n",
    "# \"\"\")\n",
    "\n",
    "# # Show the results\n",
    "# year_wise_tickets.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ee2ef9-5261-4a9e-ad29-5030ec9a685c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n|  plate_id|ticket_count|\n+----------+------------+\n|BLANKPLATE|       18911|\n|   86145MM|        1493|\n|   12125MJ|        1427|\n|   83460MH|        1314|\n|   91665MC|        1297|\n|   96091MA|        1233|\n|   96594MJ|        1150|\n|   12863KA|        1143|\n|   80279MM|        1136|\n|   69895PC|        1068|\n|   32718MM|        1068|\n|   56253MG|        1066|\n|   69894PC|        1047|\n|        NS|        1041|\n|   86107MM|        1039|\n|   29177ML|        1038|\n|   41950JX|        1002|\n|   72479ML|        1002|\n|   80302MM|         994|\n|   80252MM|         993|\n+----------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Check the Plate Id for any erroneous data and if exists remove the erroneous data\n",
    "# Check the tickets issued based on plate ids\n",
    "\n",
    "check_plate_id = spark.sql(\"select Plate_ID as plate_id, count(*) as ticket_count \\\n",
    "                          from parking \\\n",
    "                          group by plate_id \\\n",
    "                          having count(*) > 1 \\\n",
    "                          order by ticket_count desc\")\n",
    "\n",
    "check_plate_id.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c531c908-1e68-4195-b78c-38684d6d8976",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Remove the rows containing value as BLANKPLATE for plate_id\n",
    "\n",
    "df8 = df8.filter((col(\"plate_id\") != \"BLANKPLATE\") & (col(\"plate_id\") != \"NS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e87bcf-b91e-463b-8b44-89a885fb50cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|   count|\n+--------+\n|27245611|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# update the temp table with the current data\n",
    "\n",
    "df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# Check the count now \n",
    "\n",
    "spark.sql(\"select count(*) as count FROM parking\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aa73ee9-e8c5-4147-bebb-6442d68f68f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8.write.format(\"delta\").mode('overWrite').option('overwriteSchema', 'true').save('abfs://silver@azdac24we.dfs.core.windows.net/raw/track3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f449fc0c-d43c-44bd-a595-46800b9ab467",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a204946-d1cb-49e1-aba4-0aebf96d89a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df8.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e94493-9414-4a80-b3be-fee1a0c345dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n|reg_state|ticket_count|\n+---------+------------+\n|       NY|    20449811|\n|       NJ|     2576145|\n|       PA|      887296|\n|       FL|      563063|\n|       CT|      428090|\n|       TX|      249458|\n|       IN|      209848|\n|       MA|      202535|\n|       VA|      200552|\n|       NC|      164527|\n|       MD|      159385|\n|       GA|      152005|\n|       CA|       88439|\n|       IL|       85263|\n|       OH|       81084|\n|       AZ|       77472|\n|       SC|       68272|\n|       ME|       61215|\n|       DE|       45365|\n|       MI|       38490|\n|       TN|       37801|\n|       MN|       36186|\n|       AL|       35244|\n|       WA|       30771|\n|       RI|       29870|\n|       VT|       27622|\n|       NH|       24157|\n|       OK|       23729|\n|       OR|       17986|\n|       WI|       17721|\n|       GV|       17587|\n|       CO|       15623|\n|       MO|       15185|\n|       MS|       15026|\n|       KY|       11837|\n|       LA|       11690|\n|       ON|       10129|\n|       WV|        7971|\n|       SD|        7816|\n|       DC|        7292|\n|       AR|        6252|\n|       QB|        6205|\n|       IA|        5617|\n|       NV|        5212|\n|       MT|        4837|\n|       UT|        4353|\n|       NM|        3923|\n|       KS|        3703|\n|       NE|        3446|\n|       DP|        3310|\n|       ID|        3082|\n|       WY|        1374|\n|       AK|        1149|\n|       ND|        1032|\n|       HI|         724|\n|       PR|         679|\n|       AB|         326|\n|       NS|         226|\n|       BC|         182|\n|       NB|         141|\n|       FO|          95|\n|       MB|          91|\n|       SK|          47|\n|       PE|          30|\n|       MX|           9|\n|       NF|           3|\n|       NT|           3|\n|       YT|           2|\n+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# update the temp table with the current data\n",
    "\n",
    "df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# Check the ticket counts based in registered state\n",
    "registered_state_wise_tickets = spark.sql(\"select registration_state as reg_state, count(*) as ticket_count \\\n",
    "                               from parking \\\n",
    "                               group by reg_state \\\n",
    "                               order by ticket_count desc\")\n",
    "\n",
    "registered_state_wise_tickets.show(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f23712c8-d054-4952-bfb4-1e4237d0ab7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|count|\n+-----+\n|   68|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Count of distinct Registration states\n",
    "\n",
    "spark.sql(\"select count(distinct registration_state) as count from parking\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0156458e-c964-4e97-9471-3225b6281176",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## replacing the Registration state having '99' value with 'NY' state (state with max entries)\n",
    "\n",
    "from pyspark.sql.functions import when,lit\n",
    "\n",
    "\n",
    "df8 = df8.withColumn('registration_state', \\\n",
    "                                     when(df8[\"registration_state\"] == \"99\", lit('NY'))\\\n",
    "                                     .otherwise(df8[\"registration_state\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "834bc5b9-843a-4831-af35-aa9adf0c0074",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|count|\n+-----+\n|   68|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Count of distinct Registration states\n",
    "\n",
    "spark.sql(\"select count(distinct registration_state) as count from parking\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbab3074-4ae3-4b7c-87c6-f8305f7fd70f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+\n|registration_states|ticket_frequency|\n+-------------------+----------------+\n|                 NY|        20399004|\n|                 NJ|         2576145|\n|                 PA|          887296|\n|                 FL|          563063|\n|                 CT|          392051|\n+-------------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# update the temp table with the current data\n",
    "\n",
    "df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# How often does each plate_type occur? Display the frequency of the top five plate_type\n",
    "\n",
    "registration_state_frequency = spark.sql(\"select registration_state as registration_states, count(*) as ticket_frequency \\\n",
    "                                      from parking \\\n",
    "                                      group by registration_states \\\n",
    "                                      order by ticket_frequency desc \\\n",
    "                                      limit 5\")\n",
    "registration_state_frequency.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c504157-ce5c-494d-88f9-2ac080b4b92c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a dataframe with the registration_state_frequency\n",
    "\n",
    "registration_state_frequency_df = registration_state_frequency.toPandas()\n",
    "\n",
    "# plot a graph\n",
    "plt.clf()\n",
    "registration_state_frequency_df.plot(x='registration_states', y='ticket_frequency', kind='bar')\n",
    "plt.title('Frequency Of Top 5 Registration State', fontsize = 14)\n",
    "plt.xlabel(\"Registration State\", fontsize = 12)\n",
    "plt.ylabel(\"Ticket Frequency\", fontsize = 12)\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "634a4d15-cd91-4aae-bec3-b1111c3d6939",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8.write.format(\"delta\").mode('overWrite').option('overwriteSchema', 'true').save('abfs://silver@azdac24we.dfs.core.windows.net/raw/track4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3bf7c30-1f0d-4d5c-88e8-98a9d7345fc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n|month|ticket_count|\n+-----+------------+\n|    7|     6066813|\n|    8|     5591323|\n|   10|     4143123|\n|    9|     3946531|\n|   11|     3574593|\n|   12|     1796176|\n|    6|     1232258|\n|    1|      879849|\n|    5|       14069|\n|    4|         423|\n|    3|         358|\n|    2|          95|\n+-----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Let us check which month in the year 2017 has maximum summons\n",
    "df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "month_wise_tickets = spark.sql(\"select month(issue_date) as month, count(*) as ticket_count \\\n",
    "                               from parking \\\n",
    "                               group by month(issue_date) \\\n",
    "                               order by ticket_count desc\")\n",
    "\n",
    "month_wise_tickets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0463b7b-8fd5-4e40-a808-e4c28f6b63bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a dataframe with the month wise tickets data\n",
    "\n",
    "month_wise_tickets_df = month_wise_tickets.toPandas()\n",
    "\n",
    "# plot a graph\n",
    "plt.clf()\n",
    "month_wise_tickets_df.plot(x= 'month', y='ticket_count', kind='bar', color='blue')\n",
    "plt.title('Month wise tickets count', fontsize = 14)\n",
    "plt.xlabel(\"Month\", fontsize = 12)\n",
    "plt.ylabel(\"Ticket Count\", fontsize = 12)\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e758d96-039a-48f4-8cba-985f760e8e15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n|plateid|ticket_count|\n+-------+------------+\n|86145MM|        1493|\n|12125MJ|        1427|\n|83460MH|        1314|\n|91665MC|        1297|\n|96091MA|        1233|\n+-------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Top 5 Plate Ids with maximum violations\n",
    "\n",
    "top_5_plate_ids = spark.sql(\"select plate_id as plateid, count(*) as ticket_count \\\n",
    "                             from parking \\\n",
    "                             group by plateid \\\n",
    "                             order by ticket_count desc \\\n",
    "                             limit 5\")\n",
    "top_5_plate_ids.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a52b0fd-ee80-4b73-bf71-0460d9bd202b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a dataframe with the month wise tickets data\n",
    "\n",
    "top_5_plate_ids_df = top_5_plate_ids.toPandas()\n",
    "\n",
    "# plot a graph\n",
    "plt.clf()\n",
    "top_5_plate_ids_df.plot(x='plateid', y='ticket_count', kind='bar', color='brown')\n",
    "plt.title(\"Top 5 License Plate Id with Maximum Parking Violations\", fontsize = 14)\n",
    "plt.xlabel(\"Plate Id\", fontsize = 12)\n",
    "plt.ylabel(\"Ticket Count\", fontsize = 12)\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f07e078-98b5-4651-b23d-813660decc6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8.write.format(\"delta\").mode('overWrite').option('overwriteSchema', 'true').save('abfs://silver@azdac24we.dfs.core.windows.net/raw/track5/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac2e55b1-b8b9-4e5f-9a7d-58dc6604f843",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+\n|vehicle_body_types|ticket_frequency|\n+------------------+----------------+\n|              SUBN|        11324414|\n|              4DSD|         7531135|\n|               VAN|         2449145|\n|              DELV|          813645|\n|              PICK|          777202|\n+------------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# Display the frequency of the top five Vehicle Body Type getting a parking ticket\n",
    "\n",
    "vehicle_body_type_frequency = spark.sql(\"select vehicle_body_type as vehicle_body_types, count(*) as ticket_frequency \\\n",
    "                                      from parking \\\n",
    "                                      group by vehicle_body_types \\\n",
    "                                      order by ticket_frequency desc \\\n",
    "                                      limit 5\")\n",
    "\n",
    "vehicle_body_type_frequency.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9390f120-43b4-43a5-a355-4fda9e2d994a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create a dataframe with the vehicle_body_type_frequency\n",
    "\n",
    "vehicle_body_type_frequency_df = vehicle_body_type_frequency.toPandas()\n",
    "\n",
    "# plot a graph\n",
    "plt.clf()\n",
    "vehicle_body_type_frequency_df.head(5).plot(x='vehicle_body_types', y='ticket_frequency', kind='bar')\n",
    "plt.title('Frequency Of Top 5 Parking Violations Based On Vehicle Body Type', fontsize = 14)\n",
    "plt.xlabel(\"Vehicle Body Type\", fontsize = 12)\n",
    "plt.ylabel(\"Ticket Frequency\", fontsize = 12)\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29ce0f1f-11a6-49f8-885f-f7ce49f8a0d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d04c92b-4acd-4172-b2e5-445b6db6a3ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# Group by 'violation_code', count, and sort in descending order\n",
    "parking = parking.groupBy(\"violation_code\") \\\n",
    "    .count() \\\n",
    "    .sort(col(\"count\").desc())\n",
    "\n",
    "# Collecting the results to the driver\n",
    "results = parking.collect()\n",
    "\n",
    "# Printing the results\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9911450-bb8b-49f0-bcd9-a110f65c1f5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove the rows containing value as 0 for violation_code\n",
    "\n",
    "df8 = df8[df8.violation_code != 0]\n",
    "df8.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d604fe2-5979-4886-86a1-9b83a9172364",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8.write.format(\"delta\").mode('overWrite').option('overwriteSchema', 'true').save('abfs://silver@azdac24we.dfs.core.windows.net/raw/track6/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e806b560-5807-4d8b-9e00-e8c2b8915af0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3346a0e1-aab5-4469-a7bc-a8cfd72aa22b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n|viol_code|ticket_frequency|\n+---------+----------------+\n|       36|         8792080|\n|       21|         3182963|\n|       38|         1932369|\n|       14|         1620280|\n|       20|         1341416|\n+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# update the temp table with the current data\n",
    "\n",
    "df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# How often does each violation code occur? Display the frequency of the top five violation codes\n",
    "\n",
    "violation_code_frequency = spark.sql(\"select violation_code as viol_code, count(*) as ticket_frequency \\\n",
    "                                      from parking \\\n",
    "                                      group by viol_code \\\n",
    "                                      order by ticket_frequency desc \\\n",
    "                                      limit 5\")\n",
    "violation_code_frequency.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9973811c-1a83-420f-afdd-d35d615d6f09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a dataframe with the violation_code_frequency\n",
    "\n",
    "violation_code_frequency_df = violation_code_frequency.toPandas()\n",
    "\n",
    "# plot a graph\n",
    "plt.clf()\n",
    "violation_code_frequency_df.plot(x='viol_code', y='ticket_frequency', kind='bar')\n",
    "plt.title('Frequency Of Top 5 Violation Code', fontsize = 14)\n",
    "plt.xlabel(\"Violation Code\", fontsize = 12)\n",
    "plt.ylabel(\"Ticket Frequency\", fontsize = 12)\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90642071-1ee3-4236-b369-5292c792d015",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# Assigning the DataFrame to the 'parking' variable\n",
    "parking = spark.sql(\"SELECT * FROM parking\")\n",
    "\n",
    "# Group by 'vehicle_make', count, and sort in descending order\n",
    "parking = parking.groupBy(\"vehicle_make\") \\\n",
    "    .count() \\\n",
    "    .sort(col(\"count\").desc())\n",
    "\n",
    "# Collecting the results to the driver\n",
    "results = parking.collect()\n",
    "\n",
    "# Printing the results\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ceee2f9-1e20-4a70-b10c-ca270a788f9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n|vehicles_make|ticket_frequency|\n+-------------+----------------+\n|        HONDA|         3189044|\n|        TOYOT|         3054095|\n|         FORD|         2621148|\n|        NISSA|         2364510|\n|        CHEVR|         1484133|\n+-------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# update the temp table with the current data\n",
    "\n",
    "df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# How often does each vehicle_make occur? Display the frequency of the top five vehicle_make\n",
    "\n",
    "vehicle_make_frequency = spark.sql(\"select vehicle_make as vehicles_make, count(*) as ticket_frequency \\\n",
    "                                      from parking \\\n",
    "                                      group by vehicles_make \\\n",
    "                                      order by ticket_frequency desc \\\n",
    "                                      limit 5\")\n",
    "vehicle_make_frequency.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "548b3987-4604-4ad5-af38-de305f51b762",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a dataframe with the violation_code_frequency\n",
    "\n",
    "vehicle_make_frequency_df = vehicle_make_frequency.toPandas()\n",
    "\n",
    "# plot a graph\n",
    "plt.clf()\n",
    "vehicle_make_frequency_df.plot(x='vehicles_make', y='ticket_frequency', kind='bar')\n",
    "plt.title('Frequency Of Top 5 Vehicle Make', fontsize = 14)\n",
    "plt.xlabel(\"Vehicle Make\", fontsize = 12)\n",
    "plt.ylabel(\"Ticket Frequency\", fontsize = 12)\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68a6c14c-0ddb-4f55-a171-2b8c8d71a5b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8.write.format(\"delta\").mode('overWrite').option('overwriteSchema', 'true').save('abfs://silver@azdac24we.dfs.core.windows.net/raw/track8/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98438034-0505-4561-ad51-5bedc259bc73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "# df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# # Group by 'issuer_precinct', count, and sort in descending order\n",
    "# parking = parking.groupBy(\"issuer_precinct\") \\\n",
    "#     .count() \\\n",
    "#     .sort(col(\"count\").desc())\n",
    "# \n",
    "# # Collecting the results to the driver\n",
    "# results = parking.collect()\n",
    "\n",
    "# # Printing the results\n",
    "# for row in results:\n",
    "#     print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa13dd22-246d-4477-ac41-2bad5475783a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15345796"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the rows containing value as 0 for issuer_precinct\n",
    "\n",
    "df8 = df8[df8.issuer_precinct != 0]\n",
    "df8.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "108b056c-478d-488d-b024-6bc96b8a9e5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14769217"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the rows containing value as 0 for issuer_precinct\n",
    "\n",
    "df8 = df8[df8.issuer_precinct != 114]\n",
    "df8.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b09fb26-2b90-4ec1-9947-d9b78fe2eec7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n|issuers_precinct|ticket_frequency|\n+----------------+----------------+\n|              19|          772528|\n|              14|          574213|\n|              18|          537199|\n|              13|          530970|\n|               1|          452130|\n+----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# update the temp table with the current data\n",
    "\n",
    "df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# How often does each issuer_precinct occur? Display the frequency of the top five issuer_precinct\n",
    "\n",
    "issuer_precinct_frequency = spark.sql(\"select issuer_precinct as issuers_precinct, count(*) as ticket_frequency \\\n",
    "                                      from parking \\\n",
    "                                      group by issuers_precinct \\\n",
    "                                      order by ticket_frequency desc \\\n",
    "                                      limit 5\")\n",
    "issuer_precinct_frequency.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffcd08f8-3dbc-4cfd-9430-6ff0f6bbdeab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a dataframe with the violation_code_frequency\n",
    "\n",
    "issuer_precinct_frequency_df = issuer_precinct_frequency.toPandas()\n",
    "\n",
    "# plot a graph\n",
    "plt.clf()\n",
    "issuer_precinct_frequency_df.plot(x='issuers_precinct', y='ticket_frequency', kind='bar')\n",
    "plt.title('Frequency Of Top 5 Issuer Precinct', fontsize = 14)\n",
    "plt.xlabel(\"Issuer Precinct\", fontsize = 12)\n",
    "plt.ylabel(\"Ticket Frequency\", fontsize = 12)\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f06750-7fc4-4eaa-a975-baf79ac76265",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+\n|violations_code|ticket_frequency|\n+---------------+----------------+\n|             21|          109469|\n|             38|           97948|\n|             20|           80067|\n|             14|           75515|\n|             46|           69334|\n+---------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Violation code Frquency for Issuer Precinct 19 \n",
    "\n",
    "violation_code_frequency_precinct19 = spark.sql(\"select violation_code as violations_code, count(*) as ticket_frequency \\\n",
    "                                                from parking \\\n",
    "                                                where issuer_precinct = 19 \\\n",
    "                                                group by violations_code \\\n",
    "                                                order by ticket_frequency desc \\\n",
    "                                                limit 5 \")\n",
    "\n",
    "violation_code_frequency_precinct19.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d52de8d-fd2e-429d-86c6-734f3028656c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n|viol_code|ticket_frequency|\n+---------+----------------+\n|       14|          111996|\n|       69|          100935|\n|       31|          100673|\n|       47|           34618|\n|       38|           27672|\n+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Violation code Frquency for Issuer Precinct 14\n",
    "\n",
    "violation_code_frequency_precinct19 = spark.sql(\"select violation_code as viol_code, count(*) as ticket_frequency \\\n",
    "                                                from parking \\\n",
    "                                                where issuer_precinct = 14 \\\n",
    "                                                group by viol_code \\\n",
    "                                                order by ticket_frequency desc \\\n",
    "                                                limit 5 \")\n",
    "\n",
    "violation_code_frequency_precinct19.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37b0219c-4467-469b-a9dc-f2e71162b2ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n|viol_code|ticket_frequency|\n+---------+----------------+\n|       14|          165474|\n|       69|           71230|\n|       31|           58922|\n|       47|           31895|\n|       38|           25499|\n+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Violation code Frquency for Issuer Precinct 18\n",
    "\n",
    "violation_code_frequency_precinct19 = spark.sql(\"select violation_code as viol_code, count(*) as ticket_frequency \\\n",
    "                                                from parking \\\n",
    "                                                where issuer_precinct = 18 \\\n",
    "                                                group by viol_code \\\n",
    "                                                order by ticket_frequency desc \\\n",
    "                                                limit 5 \")\n",
    "\n",
    "violation_code_frequency_precinct19.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17bd4b49-94ef-4e62-b5c4-a1f6739f684b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n|viol_code|ticket_frequency|\n+---------+----------------+\n|       31|           84298|\n|       69|           77903|\n|       47|           49332|\n|       38|           45587|\n|       14|           42054|\n+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Violation code Frquency for Issuer Precinct 13 \n",
    "\n",
    "violation_code_frequency_precinct19 = spark.sql(\"select violation_code as viol_code, count(*) as ticket_frequency \\\n",
    "                                                from parking \\\n",
    "                                                where issuer_precinct = 13 \\\n",
    "                                                group by viol_code \\\n",
    "                                                order by ticket_frequency desc \\\n",
    "                                                limit 5 \")\n",
    "\n",
    "violation_code_frequency_precinct19.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2f59cb9-8865-430e-a0d5-f9391c1b1456",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n|viol_code|ticket_frequency|\n+---------+----------------+\n|       20|           88988|\n|       14|           73496|\n|       16|           43583|\n|       31|           26943|\n|       38|           25638|\n+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Violation code Frquency for Issuer Precinct 1\n",
    "\n",
    "violation_code_frequency_precinct19 = spark.sql(\"select violation_code as viol_code, count(*) as ticket_frequency \\\n",
    "                                                from parking \\\n",
    "                                                where issuer_precinct = 1 \\\n",
    "                                                group by viol_code \\\n",
    "                                                order by ticket_frequency desc \\\n",
    "                                                limit 5 \")\n",
    "\n",
    "violation_code_frequency_precinct19.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d22b3b-07d5-46f6-b8d3-bd5598a9b9df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+\n|violations_code|ticket_frequency|\n+---------------+----------------+\n|             14|          468535|\n|             31|          302858|\n|             69|          289999|\n|             20|          245839|\n|             38|          222344|\n+---------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Common violation Codes across issuer precincts 19, 14, 18, 13 and 1\n",
    "\n",
    "common_precincts_violation_codes = spark.sql(\"select violation_code as violations_code , count(*) as ticket_frequency \\\n",
    "                                              from parking \\\n",
    "                                              where issuer_precinct in (19, 14, 18, 13, 1) \\\n",
    "                                              group by violations_code \\\n",
    "                                              order by ticket_frequency desc \\\n",
    "                                              limit 5 \")\n",
    "\n",
    "common_precincts_violation_codes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05cf3b40-9c22-423e-a9cf-047d263daecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a dataframe with the common_precincts_violation_codes\n",
    "\n",
    "common_precincts_violation_codes_df = common_precincts_violation_codes.toPandas()\n",
    "\n",
    "# plot a graph\n",
    "plt.clf()\n",
    "common_precincts_violation_codes_df.plot(x='violations_code', y='ticket_frequency', kind='bar')\n",
    "plt.title('Violation Codes Across Issuer Precincts 19, 14, 18, 13 and 1', fontsize = 14)\n",
    "plt.xlabel(\"Violation Codes\", fontsize = 12)\n",
    "plt.ylabel(\"Ticket Frequency\", fontsize = 12)\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "452ef9f6-9876-463c-a290-b448797e5aa2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8.write.format(\"delta\").mode('overWrite').option('overwriteSchema', 'true').save('abfs://silver@azdac24we.dfs.core.windows.net/raw/track9/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c42cd5a-49cc-486d-941d-d3604d8e9219",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------------+------+\n|summons_number|issue_date|violation_code|Season|\n+--------------+----------+--------------+------+\n|    1452062766|2019-06-04|            99|summer|\n|    1459077660|2019-06-12|            47|summer|\n|    1459050370|2019-06-12|             9|summer|\n|    1459048751|2019-06-12|             9|summer|\n|    1457920281|2019-06-12|            67|summer|\n|    1448519330|2019-06-12|            48|summer|\n|    1447678801|2019-06-12|            14|summer|\n|    1447171226|2019-06-12|            19|summer|\n|    1456735743|2019-06-12|            40|summer|\n|    1459046924|2019-06-13|             9|summer|\n+--------------+----------+--------------+------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# First let us divide the year into seasons based on the Issue Date\n",
    "\n",
    "# We shall divide the 4 seasons based on Issue Date as follows\n",
    "\n",
    "#     Spring = March to May\n",
    "#     Summer = June to August\n",
    "#     Autumn = September to November\n",
    "#     winter = December to February\n",
    "\n",
    "seasons = spark.sql(\"select summons_number, issue_date, violation_code,  \\\n",
    "                        case \\\n",
    "                            when MONTH(TO_DATE(issue_date, 'MM/dd/yyyy')) between 03 and 05 \\\n",
    "                                then 'spring' \\\n",
    "                            when MONTH(TO_DATE(issue_date, 'MM/dd/yyyy')) between 06 and 08 \\\n",
    "                                then 'summer' \\\n",
    "                            when MONTH(TO_DATE(issue_date, 'MM/dd/yyyy')) between 09 and 11 \\\n",
    "                                then 'autumn' \\\n",
    "                            when MONTH(TO_DATE(issue_date, 'MM/dd/yyyy')) in (1,2,12) \\\n",
    "                                then 'winter' \\\n",
    "                            else 'unknown' \\\n",
    "                        end as Season \\\n",
    "                        from parking\")\n",
    "\n",
    "seasons.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1875cd7-86c8-4f18-8727-306f39a4e7d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create/Replace a Temp View\n",
    "\n",
    "seasons.createOrReplaceTempView(\"seasons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d5ca0ec-5739-4493-99ae-fa17349e7c6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n|season|ticket_frequency|\n+------+----------------+\n|summer|         6763794|\n|autumn|         6587238|\n|winter|         1415658|\n|spring|            2527|\n+------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Frequency of tickets based on season\n",
    "\n",
    "parking_violations_on_seasons = spark.sql(\"select Season as season, count(*) as ticket_frequency \\\n",
    "                                           from seasons \\\n",
    "                                           group by season \\\n",
    "                                           order by ticket_frequency desc\")\n",
    "parking_violations_on_seasons.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd353ce-35bb-4278-bd31-8f0529f03f26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n|violations_code|violation_count|\n+---------------+---------------+\n|             14|            361|\n|             20|            321|\n|             40|            285|\n+---------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Three most commonly occuring violation for spring i.e. from March to May\n",
    "\n",
    "spring = spark.sql(\"select violation_code as violations_code, count(*) as violation_count \\\n",
    "                    from seasons \\\n",
    "                    where Season == 'spring' \\\n",
    "                    group by violations_code \\\n",
    "                    order by violation_count desc \\\n",
    "                    limit 3 \")\n",
    "spring.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2258249a-a5b7-4eca-927a-2308914d03ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n|violations_code|violation_count|\n+---------------+---------------+\n|             71|         191596|\n|             21|         171602|\n|             38|         159601|\n+---------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Three most commonly occuring violation for winter i.e. from December to February\n",
    "\n",
    "winter = spark.sql(\"select violation_code as violations_code, count(*) as violation_count \\\n",
    "                    from seasons \\\n",
    "                    where Season == 'winter' \\\n",
    "                    group by violations_code \\\n",
    "                    order by violation_count desc \\\n",
    "                    limit 3 \")\n",
    "winter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1da1205-1817-4d65-9ed3-eafc9dd3b22e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n|violations_code|violation_count|\n+---------------+---------------+\n|             21|        1134703|\n|             38|         825238|\n|             14|         713044|\n+---------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Three most commonly occuring violation for summer i.e. from June to September\n",
    "\n",
    "summer = spark.sql(\"select violation_code as violations_code, count(*) as violation_count \\\n",
    "                    from seasons \\\n",
    "                    where Season == 'summer' \\\n",
    "                    group by violations_code \\\n",
    "                    order by violation_count desc \\\n",
    "                    limit 3 \")\n",
    "summer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8aa1108-768d-42df-b593-48361fe51394",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n|violations_code|violation_count|\n+---------------+---------------+\n|             21|        1020697|\n|             38|         821168|\n|             14|         722382|\n+---------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Three most commonly occuring violation for autumn i.e. from September to November\n",
    "\n",
    "autumn = spark.sql(\"select violation_code as violations_code, count(*) as violation_count \\\n",
    "                    from seasons \\\n",
    "                    where Season == 'autumn' \\\n",
    "                    group by violations_code \\\n",
    "                    order by violation_count desc \\\n",
    "                    limit 3 \")\n",
    "autumn.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a97004cd-486b-4548-8256-73a77216404f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+\n|violations_code|ticket_frequency|\n+---------------+----------------+\n|             21|         2327055|\n|             38|         1806009|\n|             14|         1575433|\n+---------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "top_3_common_violations = spark.sql(\"select violation_code as violations_code, count(*) as ticket_frequency \\\n",
    "                                    from parking \\\n",
    "                                    group by violations_code \\\n",
    "                                    order by ticket_frequency desc \\\n",
    "                                    limit 3\")\n",
    "top_3_common_violations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90514033-a1f7-4394-bee9-6e200910ec6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+-----------+\n|violations_code|ticket_frequency|fine_amount|\n+---------------+----------------+-----------+\n|             21|         2327055|  127988025|\n|             38|         1806009|   90300450|\n|             14|         1575433|  181174795|\n+---------------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# From the above result we know the top three violation codes.\n",
    "\n",
    "# As per the website, the average prices for the three violation codes are as follows:\n",
    "#   For violation code 21 = (65 + 45)/2 = $55\n",
    "#   For violation code 38 = (65 + 35)/2 = $50\n",
    "#   For violation code 14 = (115 + 115)/2 = $115\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "common_violations_fine_amount = top_3_common_violations.withColumn(\"fine_amount\",\n",
    "    when(top_3_common_violations.violations_code == 21, top_3_common_violations.ticket_frequency * 55)\n",
    "    .when(top_3_common_violations.violations_code == 38, top_3_common_violations.ticket_frequency * 50)\n",
    "    .otherwise(top_3_common_violations.ticket_frequency * 115)\n",
    ")\n",
    "\n",
    "common_violations_fine_amount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61bdcbb1-fd31-46d6-bfe1-1f0b2729f50c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount collected for the three violation codes with maximum tickets :  [Row(sum(fine_amount)=399463270)]\n"
     ]
    }
   ],
   "source": [
    "# Total amount collected for the three violation codes with maximum tickets\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "total = common_violations_fine_amount.agg(F.sum(\"fine_amount\")).collect()\n",
    "print('Total amount collected for the three violation codes with maximum tickets : ', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf6e042-a022-4a5a-bdc7-23c4e75cc684",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+-----------+\n|violations_code|ticket_frequency|fine_amount|\n+---------------+----------------+-----------+\n|             21|         2327055|  127988025|\n+---------------+----------------+-----------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "# State the code that has the highest total collection\n",
    "\n",
    "common_violations_fine_amount.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780321b3-5467-4ae0-be17-8a3338cc98a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8.write.format(\"delta\").mode('overWrite').option('overwriteSchema', 'true').save('abfs://silver@azdac24we.dfs.core.windows.net/raw/track10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da1897f7-c47b-405e-8a47-cea6e76d769e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- summons_number: string (nullable = true)\n |-- plate_id: string (nullable = true)\n |-- registration_state: string (nullable = true)\n |-- plate_type: string (nullable = true)\n |-- issue_date: date (nullable = true)\n |-- violation_code: string (nullable = true)\n |-- vehicle_body_type: string (nullable = true)\n |-- vehicle_make: string (nullable = true)\n |-- issuer_precinct: string (nullable = true)\n |-- issuer_code: string (nullable = true)\n |-- violation_time: string (nullable = true)\n |-- violation_county: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV Reader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "df10 = spark.read.format(\"delta\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"abfs://silver@azdac24we.dfs.core.windows.net/raw/track10/\")\n",
    "\n",
    "# Show the DataFrame schema\n",
    "df10.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88033315-0d29-4419-b3a6-83e409d09f7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df10.repartition(1).write.mode(\"overwrite\").option(\"header\",'true').csv('abfs://powerbicontainer@azdac24we.dfs.core.windows.net/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e833ae2d-ef95-4a01-973b-ba03fae05598",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14769217"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df10.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4d2ab55-08e3-48bd-a9be-a46b787587c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+---------------+-----------+--------------+----------------+\n|summons_number|plate_id|registration_state|plate_type|issue_date|violation_code|vehicle_body_type|vehicle_make|issuer_precinct|issuer_code|violation_time|violation_county|\n+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+---------------+-----------+--------------+----------------+\n|    1449397270| JGG2337|                NY|       PAS|2019-06-01|            46|                O|       NISSA|            809|     953753|         0742P|               Q|\n|    1446907170|  WJ982V|                NJ|       PAS|2019-06-02|            14|             SUBN|        JEEP|              1|     161407|         0917A|              NY|\n|    1457096560|T726825C|                NY|       OMT|2019-06-05|            98|              SDN|       TOYOT|            102|     958291|         0213A|               Q|\n|    1446899421| JDP6070|                NY|       PAS|2019-06-05|            63|              SDN|       CHEVR|              2|     160664|         0525P|              BX|\n|    1455271135|  HYJK06|                FL|       PAS|2019-06-06|            27|              SDN|        AUDI|              4|     760608|         0833P|               K|\n|    1454016206|  97820W|                CT|       PAS|2019-06-07|            98|              SDN|       MITSU|            121|     944037|         0840A|               R|\n|    1455178330| JGU5379|                NY|       PAS|2019-06-11|            14|              VAN|       DODGE|            968|     241329|         0920A|              NY|\n|    1459125502| 28641MH|                NY|       COM|2019-06-12|            14|              VAN|       DODGE|            401|     961554|         0500P|              NY|\n|    1459051567| AP99638|                CT|       PAS|2019-06-12|            14|             SUBN|        FORD|            401|     955318|         0343P|              NY|\n|    1459112581| 51771JW|                NY|       COM|2019-06-12|            14|             DELV|       INTER|            401|     954190|         0536P|              NY|\n|    1459039725| 60435MD|                NY|       PAS|2019-06-12|            46|             DELV|       FRUEH|            406|     911579|         0945A|               K|\n|    1452891540|T728235C|                NY|       OMT|2019-06-12|            20|              SDN|       TOYOT|            109|     357687|         1210P|               Q|\n|    1459138454| 31412MG|                NY|       COM|2019-06-13|            14|              VAN|       FRUEH|            401|     953483|         1022A|              NY|\n|    1458942211| HXK4402|                NY|       PAS|2019-06-13|            31|             SUBN|       DODGE|            401|     956330|         1133A|              NY|\n|    1459136550|  ABDC96|                NJ|       PAS|2019-06-13|            14|             DELV|         HIN|            401|     948355|         0912A|              NY|\n|    1456697997| 91648MC|                NY|       COM|2019-06-13|            40|             DELV|       FRUEH|             34|     945725|         0535P|              NY|\n|    1459126488| 81651JN|                NY|       COM|2019-06-13|            19|             DELV|       INTER|            401|     962185|         0521P|               K|\n|    1451124260| 26472ML|                NY|       COM|2019-06-13|            78|              VAN|        FORD|             62|     963389|         1047P|               K|\n|    1449035700| HNR4585|                NY|       PAS|2019-06-13|            61|             SUBN|       DODGE|            122|     935892|         1115P|               R|\n|    1455618135| GNS6191|                NY|       PAS|2019-06-13|            14|             SUBN|        FORD|            110|     350318|         0723A|               K|\n+--------------+--------+------------------+----------+----------+--------------+-----------------+------------+---------------+-----------+--------------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4aca4f1-4f51-4432-acd6-9019997b80b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4268b6ae-ed3d-4815-a64d-c59508ee4125",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ce71dc7-2fd6-4d07-89d9-237c2cb533aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "765f6c9c-e474-4d94-aec7-abcee8323f2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60a3d292-87c6-41b9-885d-16c672efa8b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3873b22e-16b1-4d83-8cd6-31197be01997",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58489fcb-3f18-4b70-9213-2eb86e502abf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5e60c94-d655-4884-9f7c-03e84624b519",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4096a76a-c5f2-4a5a-83c0-3da154256baa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e738eeb6-3479-4806-87f3-29b76627c8ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8.write.format(\"delta\").mode('overWrite').option('overwriteSchema', 'true').save('abfs://silver@azdac24we.dfs.core.windows.net/raw/track7/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a1db92-68ba-4ba4-a4e9-eeaaa2bf6e40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- summons_number: string (nullable = true)\n |-- plate_id: string (nullable = true)\n |-- registration_state: string (nullable = true)\n |-- plate_type: string (nullable = true)\n |-- issue_date: date (nullable = true)\n |-- violation_code: string (nullable = true)\n |-- vehicle_body_type: string (nullable = true)\n |-- vehicle_make: string (nullable = true)\n |-- issuer_precinct: string (nullable = true)\n |-- issuer_code: string (nullable = true)\n |-- violation_time: string (nullable = true)\n |-- violation_county: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV Reader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "df8 = spark.read.format(\"delta\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"abfs://silver@azdac24we.dfs.core.windows.net/raw/track7/\")\n",
    "\n",
    "# Show the DataFrame schema\n",
    "df8.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "087c78fe-4f76-4b04-a13c-ecb2a3cbe2a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(plate_type='PAS', count=22190109)\nRow(plate_type='COM', count=2864295)\nRow(plate_type='OMT', count=920381)\nRow(plate_type='SRF', count=299537)\nRow(plate_type='OMS', count=268273)\nRow(plate_type='999', count=106603)\nRow(plate_type='APP', count=92184)\nRow(plate_type='MOT', count=81589)\nRow(plate_type='LMB', count=75927)\nRow(plate_type='ORG', count=63939)\nRow(plate_type='SPO', count=39299)\nRow(plate_type='MED', count=33345)\nRow(plate_type='TRC', count=25337)\nRow(plate_type='CMB', count=25094)\nRow(plate_type='OMR', count=22813)\nRow(plate_type='PSD', count=22561)\nRow(plate_type='RGL', count=16002)\nRow(plate_type='SCL', count=12839)\nRow(plate_type='TOW', count=12148)\nRow(plate_type='OML', count=10248)\nRow(plate_type='VAS', count=8958)\nRow(plate_type='ITP', count=8957)\nRow(plate_type='TRL', count=8297)\nRow(plate_type='IRP', count=6178)\nRow(plate_type='SRN', count=5132)\nRow(plate_type='DLR', count=4042)\nRow(plate_type='HIS', count=3365)\nRow(plate_type='TRA', count=2511)\nRow(plate_type='MCL', count=1950)\nRow(plate_type='STA', count=1651)\nRow(plate_type='AMB', count=1548)\nRow(plate_type='ORC', count=1138)\nRow(plate_type='OMV', count=903)\nRow(plate_type='LMC', count=864)\nRow(plate_type='PHS', count=728)\nRow(plate_type='SPC', count=704)\nRow(plate_type='AGR', count=670)\nRow(plate_type='LMA', count=609)\nRow(plate_type='NYS', count=550)\nRow(plate_type='CHC', count=394)\nRow(plate_type='AYG', count=368)\nRow(plate_type='HAM', count=346)\nRow(plate_type='RGC', count=312)\nRow(plate_type='SOS', count=270)\nRow(plate_type='BOB', count=264)\nRow(plate_type='NLM', count=243)\nRow(plate_type='MCD', count=224)\nRow(plate_type='STG', count=215)\nRow(plate_type='NYC', count=178)\nRow(plate_type='CBS', count=156)\nRow(plate_type='TMP', count=146)\nRow(plate_type='CSP', count=135)\nRow(plate_type='SUP', count=112)\nRow(plate_type='ATV', count=105)\nRow(plate_type='CMH', count=91)\nRow(plate_type='SEM', count=91)\nRow(plate_type='LTR', count=87)\nRow(plate_type='ARG', count=76)\nRow(plate_type='NYA', count=75)\nRow(plate_type='AGC', count=51)\nRow(plate_type='VPL', count=47)\nRow(plate_type='LUA', count=42)\nRow(plate_type='HIR', count=36)\nRow(plate_type='CCK', count=27)\nRow(plate_type='FAR', count=24)\nRow(plate_type='PPH', count=21)\nRow(plate_type='THC', count=18)\nRow(plate_type='HOU', count=17)\nRow(plate_type='OMO', count=16)\nRow(plate_type='ORM', count=16)\nRow(plate_type='GSM', count=14)\nRow(plate_type='CME', count=13)\nRow(plate_type='USC', count=10)\nRow(plate_type='CLG', count=9)\nRow(plate_type='OMF', count=9)\nRow(plate_type='JWV', count=9)\nRow(plate_type='FPW', count=9)\nRow(plate_type='HSM', count=9)\nRow(plate_type='BOT', count=7)\nRow(plate_type='JCL', count=7)\nRow(plate_type='JCA', count=5)\nRow(plate_type='JSC', count=5)\nRow(plate_type='WUG', count=4)\nRow(plate_type='ATD', count=4)\nRow(plate_type='USS', count=4)\nRow(plate_type='GSC', count=3)\nRow(plate_type='HAC', count=3)\nRow(plate_type='HIF', count=2)\nRow(plate_type='SNO', count=2)\nRow(plate_type='GAC', count=2)\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# # Assigning the DataFrame to the 'parking' variable\n",
    "# parking = spark.sql(\"SELECT * FROM parking\")\n",
    "\n",
    "# # Group by 'plate_type', count, and sort in descending order\n",
    "# parking = parking.groupBy(\"plate_type\") \\\n",
    "#     .count() \\\n",
    "#     .sort(col(\"count\").desc())\n",
    "\n",
    "# # Collecting the results to the driver\n",
    "# results = parking.collect()\n",
    "\n",
    "# # Printing the results\n",
    "# for row in results:\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0fefcc7-f44f-4719-bf79-85dfddf0b444",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "27139008"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Remove the rows containing value as 999 for plate_type\n",
    "\n",
    "df8 = df8[df8.plate_type != '999']\n",
    "df8.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90af83d6-3512-4c48-967c-801c357cbba8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8.write.format(\"delta\").mode('overWrite').option('overwriteSchema', 'true').save('abfs://silver@azdac24we.dfs.core.windows.net/raw/track8/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f280d9d2-bd3a-493d-adc4-d5b4b9d4b603",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n|plate_types|ticket_frequency|\n+-----------+----------------+\n|        PAS|        22190109|\n|        COM|         2864295|\n|        OMT|          920381|\n|        SRF|          299537|\n|        OMS|          268273|\n+-----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# update the temp table with the current data\n",
    "\n",
    "df8.createOrReplaceTempView(\"parking\")\n",
    "\n",
    "# How often does each plate_type occur? Display the frequency of the top five plate_type\n",
    "\n",
    "plate_type_frequency = spark.sql(\"select plate_type as plate_types, count(*) as ticket_frequency \\\n",
    "                                      from parking \\\n",
    "                                      group by plate_types \\\n",
    "                                      order by ticket_frequency desc \\\n",
    "                                      limit 5\")\n",
    "plate_type_frequency.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1882496-6ec0-4e73-8627-a91de3c79f21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a dataframe with the violation_code_frequency\n",
    "\n",
    "plate_type_frequency_df = plate_type_frequency.toPandas()\n",
    "\n",
    "# plot a graph\n",
    "plt.clf()\n",
    "plate_type_frequency_df.plot(x='plate_types', y='ticket_frequency', kind='bar')\n",
    "plt.title('Frequency Of Top 5 plate type', fontsize = 14)\n",
    "plt.xlabel(\"Plate Type\", fontsize = 12)\n",
    "plt.ylabel(\"Ticket Frequency\", fontsize = 12)\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e8a0b1-9c8a-49cf-8a99-6e7469d19cba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df8.createOrReplaceTempView(\"parking\")\n",
    "# Group by 'vehicle_body_type', count, and sort in descending order\n",
    "parking = parking.groupBy(\"vehicle_body_type\") \\\n",
    "    .count() \\\n",
    "    .sort(col(\"count\").desc())\n",
    "\n",
    "# Collecting the results to the driver\n",
    "results = parking.collect()\n",
    "\n",
    "# Printing the results\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f5b19a-b76b-4bc1-8b79-4f49feea8791",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- summons_number: string (nullable = true)\n |-- plate_id: string (nullable = true)\n |-- registration_state: string (nullable = true)\n |-- plate_type: string (nullable = true)\n |-- issue_date: date (nullable = true)\n |-- violation_code: string (nullable = true)\n |-- vehicle_body_type: string (nullable = true)\n |-- vehicle_make: string (nullable = true)\n |-- issuer_precinct: string (nullable = true)\n |-- issuer_code: string (nullable = true)\n |-- violation_time: string (nullable = true)\n |-- violation_county: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df8.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b26b95-72a2-4ff1-bf70-4352e8cf70b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df9=df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55ff8e1-a51c-4745-8e16-dc8a6f36d17c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14769217"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df9.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e7dfa99-4ce1-47e9-9c56-e9ae3afc80d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4290004865043230>, line 15\u001B[0m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Assuming df9 is your DataFrame after loading data\u001B[39;00m\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Example: df9 = spark.read...\u001B[39;00m\n",
       "\u001B[1;32m     12\u001B[0m \n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Indexing categorical columns to transform to numeric, excluding 'issue_date' (if present) and the target 'violation_code'\u001B[39;00m\n",
       "\u001B[1;32m     14\u001B[0m categoricalColumns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msummons_number\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplate_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregistration_state\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplate_type\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvehicle_body_type\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvehicle_make\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124missuer_precinct\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124missuer_code\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mviolation_time\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mviolation_county\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
       "\u001B[0;32m---> 15\u001B[0m indexers \u001B[38;5;241m=\u001B[39m [StringIndexer(inputCol\u001B[38;5;241m=\u001B[39mc, outputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m_index\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(c))\u001B[38;5;241m.\u001B[39mfit(df9) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m categoricalColumns]\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Assembling indexed features into a vector\u001B[39;00m\n",
       "\u001B[1;32m     18\u001B[0m assemblerInputs \u001B[38;5;241m=\u001B[39m [c \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_index\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m categoricalColumns]\n",
       "\n",
       "File \u001B[0;32m<command-4290004865043230>, line 15\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Assuming df9 is your DataFrame after loading data\u001B[39;00m\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Example: df9 = spark.read...\u001B[39;00m\n",
       "\u001B[1;32m     12\u001B[0m \n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Indexing categorical columns to transform to numeric, excluding 'issue_date' (if present) and the target 'violation_code'\u001B[39;00m\n",
       "\u001B[1;32m     14\u001B[0m categoricalColumns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msummons_number\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplate_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregistration_state\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplate_type\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvehicle_body_type\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvehicle_make\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124missuer_precinct\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124missuer_code\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mviolation_time\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mviolation_county\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
       "\u001B[0;32m---> 15\u001B[0m indexers \u001B[38;5;241m=\u001B[39m [\u001B[43mStringIndexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputCol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutputCol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{0}\u001B[39;49;00m\u001B[38;5;124;43m_index\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf9\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m categoricalColumns]\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Assembling indexed features into a vector\u001B[39;00m\n",
       "\u001B[1;32m     18\u001B[0m assemblerInputs \u001B[38;5;241m=\u001B[39m [c \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_index\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m categoricalColumns]\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n",
       "\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n",
       "\u001B[1;32m    208\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:380\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    379\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n",
       "\u001B[0;32m--> 380\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    381\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n",
       "\u001B[1;32m    382\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:377\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    374\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    376\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n",
       "\u001B[0;32m--> 377\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:224\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 224\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    226\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1471.fit.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 208.0 failed 4 times, most recent failure: Lost task 0.3 in stage 208.0 (TID 425) (10.139.64.4 executor driver): org.apache.spark.SparkException: [KRYO_BUFFER_OVERFLOW] Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864\n",
       "Serialization trace:\n",
       "_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp). To avoid this, increase \"spark.kryoserializer.buffer.max\" value. SQLSTATE: 54006\n",
       "\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:499)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.aggregate.ComplexTypedAggregateExpression.eval(TypedAggregateExpression.scala:262)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:763)\n",
       "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:260)\n",
       "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:99)\n",
       "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:34)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:89)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:201)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:186)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:151)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:145)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:940)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:104)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:943)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:835)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\n",
       "Serialization trace:\n",
       "_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp)\n",
       "\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n",
       "\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeBytes(UnsafeOutput.java:384)\n",
       "\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeLongs(UnsafeOutput.java:331)\n",
       "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:130)\n",
       "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:119)\n",
       "\tat com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:629)\n",
       "\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:86)\n",
       "\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)\n",
       "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
       "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n",
       "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n",
       "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
       "\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:491)\n",
       "\t... 43 more\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3639)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3561)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3548)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3548)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1529)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1529)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1529)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3885)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3797)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3785)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1253)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1241)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2994)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:303)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:299)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$collect$1(Collector.scala:384)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:381)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:122)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:131)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:94)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:90)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:78)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:544)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:535)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$2(ResultCacheManager.scala:550)\n",
       "\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:607)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1147)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$5(SQLExecution.scala:690)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:690)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:689)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:688)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withOptimisticTransaction(SQLExecution.scala:709)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:687)\n",
       "\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:134)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:90)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:67)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:131)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:134)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: [KRYO_BUFFER_OVERFLOW] Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864\n",
       "Serialization trace:\n",
       "_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp). To avoid this, increase \"spark.kryoserializer.buffer.max\" value. SQLSTATE: 54006\n",
       "\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:499)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.aggregate.ComplexTypedAggregateExpression.eval(TypedAggregateExpression.scala:262)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:763)\n",
       "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:260)\n",
       "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:99)\n",
       "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:34)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:89)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:201)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:186)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:151)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:145)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:940)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:104)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:943)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:835)\n",
       "\t... 3 more\n",
       "Caused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\n",
       "Serialization trace:\n",
       "_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp)\n",
       "\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n",
       "\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeBytes(UnsafeOutput.java:384)\n",
       "\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeLongs(UnsafeOutput.java:331)\n",
       "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:130)\n",
       "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:119)\n",
       "\tat com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:629)\n",
       "\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:86)\n",
       "\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)\n",
       "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
       "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n",
       "\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n",
       "\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n",
       "\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:491)\n",
       "\t... 43 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o1471.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 208.0 failed 4 times, most recent failure: Lost task 0.3 in stage 208.0 (TID 425) (10.139.64.4 executor driver): org.apache.spark.SparkException: [KRYO_BUFFER_OVERFLOW] Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864\nSerialization trace:\n_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp). To avoid this, increase \"spark.kryoserializer.buffer.max\" value. SQLSTATE: 54006\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:499)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.aggregate.ComplexTypedAggregateExpression.eval(TypedAggregateExpression.scala:262)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:763)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:260)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:99)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:34)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:89)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:201)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:186)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:151)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:145)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:940)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:104)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:943)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:835)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\nSerialization trace:\n_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp)\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeBytes(UnsafeOutput.java:384)\n\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeLongs(UnsafeOutput.java:331)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:130)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:119)\n\tat com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:629)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:86)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:491)\n\t... 43 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3639)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3561)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3548)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3548)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1529)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1529)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3885)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3797)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3785)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1253)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1241)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2994)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:303)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:299)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$collect$1(Collector.scala:384)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:381)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:122)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:131)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:94)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:90)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:78)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:544)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:535)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$2(ResultCacheManager.scala:550)\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:607)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1147)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$5(SQLExecution.scala:690)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:690)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:689)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:688)\n\tat org.apache.spark.sql.execution.SQLExecution$.withOptimisticTransaction(SQLExecution.scala:709)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:687)\n\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:134)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:90)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:67)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:131)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:134)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [KRYO_BUFFER_OVERFLOW] Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864\nSerialization trace:\n_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp). To avoid this, increase \"spark.kryoserializer.buffer.max\" value. SQLSTATE: 54006\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:499)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.aggregate.ComplexTypedAggregateExpression.eval(TypedAggregateExpression.scala:262)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:763)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:260)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:99)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:34)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:89)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:201)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:186)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:151)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:145)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:940)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:104)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:943)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:835)\n\t... 3 more\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\nSerialization trace:\n_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp)\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeBytes(UnsafeOutput.java:384)\n\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeLongs(UnsafeOutput.java:331)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:130)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:119)\n\tat com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:629)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:86)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:491)\n\t... 43 more\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o1471.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 208.0 failed 4 times, most recent failure: Lost task 0.3 in stage 208.0 (TID 425) (10.139.64.4 executor driver): org.apache.spark.SparkException: [KRYO_BUFFER_OVERFLOW] Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864\nSerialization trace:\n_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp). To avoid this, increase \"spark.kryoserializer.buffer.max\" value. SQLSTATE: 54006\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:499)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.aggregate.ComplexTypedAggregateExpression.eval(TypedAggregateExpression.scala:262)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:763)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:260)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:99)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:34)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:89)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:201)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:186)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:151)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:145)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:940)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:104)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:943)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:835)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\nSerialization trace:\n_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp)\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeBytes(UnsafeOutput.java:384)\n\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeLongs(UnsafeOutput.java:331)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:130)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:119)\n\tat com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:629)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:86)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:491)\n\t... 43 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3639)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3561)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3548)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3548)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1529)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1529)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3885)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3797)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3785)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1253)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1241)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2994)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:303)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:299)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$collect$1(Collector.scala:384)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:381)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:122)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:131)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:94)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:90)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:78)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:544)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:535)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$2(ResultCacheManager.scala:550)\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:607)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1147)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$5(SQLExecution.scala:690)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:690)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:689)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:688)\n\tat org.apache.spark.sql.execution.SQLExecution$.withOptimisticTransaction(SQLExecution.scala:709)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:687)\n\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:134)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:90)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:67)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:131)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:134)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [KRYO_BUFFER_OVERFLOW] Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864\nSerialization trace:\n_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp). To avoid this, increase \"spark.kryoserializer.buffer.max\" value. SQLSTATE: 54006\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:499)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.aggregate.ComplexTypedAggregateExpression.eval(TypedAggregateExpression.scala:262)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:763)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:260)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:99)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:34)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:89)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:201)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:186)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:151)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:145)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:940)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:104)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:943)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:835)\n\t... 3 more\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\nSerialization trace:\n_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp)\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeBytes(UnsafeOutput.java:384)\n\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeLongs(UnsafeOutput.java:331)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:130)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:119)\n\tat com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:629)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:86)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:491)\n\t... 43 more\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-4290004865043230>, line 15\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Assuming df9 is your DataFrame after loading data\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Example: df9 = spark.read...\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Indexing categorical columns to transform to numeric, excluding 'issue_date' (if present) and the target 'violation_code'\u001B[39;00m\n\u001B[1;32m     14\u001B[0m categoricalColumns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msummons_number\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplate_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregistration_state\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplate_type\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvehicle_body_type\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvehicle_make\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124missuer_precinct\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124missuer_code\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mviolation_time\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mviolation_county\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 15\u001B[0m indexers \u001B[38;5;241m=\u001B[39m [StringIndexer(inputCol\u001B[38;5;241m=\u001B[39mc, outputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m_index\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(c))\u001B[38;5;241m.\u001B[39mfit(df9) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m categoricalColumns]\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Assembling indexed features into a vector\u001B[39;00m\n\u001B[1;32m     18\u001B[0m assemblerInputs \u001B[38;5;241m=\u001B[39m [c \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_index\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m categoricalColumns]\n",
        "File \u001B[0;32m<command-4290004865043230>, line 15\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Assuming df9 is your DataFrame after loading data\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Example: df9 = spark.read...\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Indexing categorical columns to transform to numeric, excluding 'issue_date' (if present) and the target 'violation_code'\u001B[39;00m\n\u001B[1;32m     14\u001B[0m categoricalColumns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msummons_number\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplate_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregistration_state\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplate_type\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvehicle_body_type\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvehicle_make\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124missuer_precinct\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124missuer_code\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mviolation_time\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mviolation_county\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 15\u001B[0m indexers \u001B[38;5;241m=\u001B[39m [\u001B[43mStringIndexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputCol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutputCol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{0}\u001B[39;49;00m\u001B[38;5;124;43m_index\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf9\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m categoricalColumns]\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Assembling indexed features into a vector\u001B[39;00m\n\u001B[1;32m     18\u001B[0m assemblerInputs \u001B[38;5;241m=\u001B[39m [c \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_index\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m categoricalColumns]\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    208\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:380\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[0;32m--> 380\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    381\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[1;32m    382\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:377\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 377\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:224\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 224\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    226\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1471.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 208.0 failed 4 times, most recent failure: Lost task 0.3 in stage 208.0 (TID 425) (10.139.64.4 executor driver): org.apache.spark.SparkException: [KRYO_BUFFER_OVERFLOW] Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864\nSerialization trace:\n_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp). To avoid this, increase \"spark.kryoserializer.buffer.max\" value. SQLSTATE: 54006\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:499)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.aggregate.ComplexTypedAggregateExpression.eval(TypedAggregateExpression.scala:262)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:763)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:260)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:99)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:34)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:89)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:201)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:186)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:151)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:145)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:940)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:104)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:943)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:835)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\nSerialization trace:\n_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp)\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeBytes(UnsafeOutput.java:384)\n\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeLongs(UnsafeOutput.java:331)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:130)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:119)\n\tat com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:629)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:86)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:491)\n\t... 43 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3639)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3561)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3548)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3548)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1529)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1529)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3885)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3797)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3785)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1253)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1241)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2994)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:303)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:299)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$collect$1(Collector.scala:384)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:381)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:122)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:131)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:94)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:90)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:78)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:544)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:535)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$2(ResultCacheManager.scala:550)\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:607)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1147)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$5(SQLExecution.scala:690)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:690)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:689)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:688)\n\tat org.apache.spark.sql.execution.SQLExecution$.withOptimisticTransaction(SQLExecution.scala:709)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:687)\n\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:134)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:90)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:67)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:131)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:134)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [KRYO_BUFFER_OVERFLOW] Kryo serialization failed: Buffer overflow. Available: 0, required: 67108864\nSerialization trace:\n_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp). To avoid this, increase \"spark.kryoserializer.buffer.max\" value. SQLSTATE: 54006\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:499)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.aggregate.ComplexTypedAggregateExpression.eval(TypedAggregateExpression.scala:262)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:763)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:260)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:99)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:34)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:89)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:201)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:186)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:151)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:145)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:940)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:104)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:943)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:835)\n\t... 3 more\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 0, required: 67108864\nSerialization trace:\n_values$mcJ$sp (org.apache.spark.util.collection.OpenHashMap$mcJ$sp)\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeBytes(UnsafeOutput.java:384)\n\tat com.esotericsoftware.kryo.io.UnsafeOutput.writeLongs(UnsafeOutput.java:331)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:130)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.write(DefaultArraySerializers.java:119)\n\tat com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:629)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:86)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:491)\n\t... 43 more\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"RandomForestClassifierExample\").getOrCreate()\n",
    "\n",
    "# Assuming df9 is your DataFrame after loading data\n",
    "# Example: df9 = spark.read...\n",
    "\n",
    "# Indexing categorical columns to transform to numeric, excluding 'issue_date' (if present) and the target 'violation_code'\n",
    "categoricalColumns = ['summons_number', 'plate_id', 'registration_state', 'plate_type', 'vehicle_body_type', 'vehicle_make', 'issuer_precinct', 'issuer_code', 'violation_time', 'violation_county']\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=\"{0}_index\".format(c)).fit(df9) for c in categoricalColumns]\n",
    "\n",
    "# Assembling indexed features into a vector\n",
    "assemblerInputs = [c + \"_index\" for c in categoricalColumns]\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "# Indexing the label column (violation_code)\n",
    "labelIndexer = StringIndexer(inputCol=\"violation_code\", outputCol=\"label\").fit(df9)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline = Pipeline(stages=indexers + [assembler, labelIndexer, rf])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "(trainingData, testData) = df9.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = %g\" % accuracy)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62a49555-adc8-4c04-af5c-a2dcf81c530a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2126595925838359,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "NYC_PARKING_VIOLATION_FINAL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
